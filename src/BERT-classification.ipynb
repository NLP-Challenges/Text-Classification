{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Callback\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Classifier\n",
    "\n",
    "Our BERT classifier for the Data Chatbot.\n",
    "\n",
    "**Relevant Resources**\n",
    "\n",
    "- https://docs.wandb.ai/guides/integrations/lightning#logger-arguments\n",
    "- https://pytorch-lightning.readthedocs.io/en/0.9.0/hyperparameters.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.labels = dataframe.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split()) # Removes any extra whitespace\n",
    "\n",
    "        # https://huggingface.co/docs/transformers/v4.34.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True, # Add '[CLS]' and '[SEP]', default True\n",
    "            max_length=self.max_len, # Maximum length to use by one of the truncation/padding parameters\n",
    "            padding='max_length', # Pad to a maximum length specified with the argument max_length\n",
    "            truncation=True, # Truncate to a maximum length specified with the argument max_length\n",
    "        )\n",
    "        ids = inputs['input_ids'] # Indices of input sequence tokens in the vocabulary\n",
    "        mask = inputs['attention_mask'] # Mask to avoid performing attention on padding token indices\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "\n",
    "# Load data and return DataLoader\n",
    "def get_dataloader(df, tokenizer, max_len=None, batch_size=32, shuffle=True, nobatch=False):\n",
    "    \"\"\"\n",
    "    Loads data into a PyTorch DataLoader object.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The data frame containing the text and labels.\n",
    "    - tokenizer (Tokenizer): The tokenizer to be used.\n",
    "    - max_len (int, optional): The maximum length for the tokenized sequences. Defaults to None (model's limitation).\n",
    "    - batch_size (int, optional): The size of each batch. Defaults to 32.\n",
    "    - shuffle (bool, optional): Whether to shuffle the data. Defaults to True.\n",
    "    - nobatch (bool, optional): Whether to disable batching. If True, batch_size will be set to the length of df. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    - DataLoader: A PyTorch DataLoader object containing the tokenized data.\n",
    "\n",
    "    Notes:\n",
    "    - The label mapping {'harm': 0, 'question': 1, 'concern': 2} is applied to the labels in df.\n",
    "    \"\"\"\n",
    "    label_mapping = {'harm': 0, 'question': 1, 'concern': 2}\n",
    "    df['label'] = df['label'].map(label_mapping)\n",
    "    dataset = ClassifierDataset(df, tokenizer, max_len)\n",
    "\n",
    "    # Handle nobatch\n",
    "    batch_size = batch_size if not nobatch else df.__len__()\n",
    "    print(f\"DataLoader | No Batch: {nobatch}; Batch Size: {batch_size}\")\n",
    "\n",
    "    # Create DataLoader\n",
    "    params = {'batch_size': batch_size, 'shuffle': shuffle, 'num_workers': 0}\n",
    "    data_loader = DataLoader(dataset, **params)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(pl.LightningModule, PyTorchModelHubMixin):\n",
    "    def __init__(self, hparams):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "\n",
    "        # Save hyperparameters\n",
    "        self.hparams.update(hparams)\n",
    "        self.__configure_from_hyperparams()\n",
    "\n",
    "        self.model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)\n",
    "    \n",
    "    def forward(self, ids, mask):\n",
    "        output = self.model(ids, attention_mask=mask)\n",
    "        return output.logits\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        return self.__step(batch, batch_nb, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        return self.__step(batch, batch_nb, 'val')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-5)\n",
    "\n",
    "    def save_pretrained(self, *args, **kwargs):\n",
    "        # Forward all arguments to the inner Bert model's save_pretrained method\n",
    "        self.model.save_pretrained(*args, **kwargs)\n",
    "\n",
    "    def push_to_hub(self, *args, **kwargs):\n",
    "        # Forward all arguments to the inner Bert model's push_to_hub method\n",
    "        self.model.push_to_hub(*args, **kwargs)\n",
    "\n",
    "    def __configure_from_hyperparams(self):\n",
    "        # Set N/A hyperparameters to default values\n",
    "        self.max_len = self.hparams.get(\"max_len\", 100)\n",
    "        self.batch_size =  self.hparams.get(\"batch_size\", 32)\n",
    "\n",
    "    def __step(self, batch, batch_idx, stage):\n",
    "        preds, loss, accuracy, f1 = self.__get_preds_loss_accuracy(batch)\n",
    "        \n",
    "        self.log(\n",
    "            f'{stage}/accuracy',\n",
    "            accuracy,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            f'{stage}/f1',\n",
    "            f1,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(f'{stage}/loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def __get_preds_loss_accuracy(self, batch):\n",
    "        # Helper function to get predictions and loss\n",
    "        ids = batch['ids']\n",
    "        mask = batch['mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        preds = self(ids, mask)\n",
    "        loss = torch.nn.CrossEntropyLoss()(preds, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(preds, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        total = labels.size(0)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        # Calculate F1 score\n",
    "        labels_cpu = labels.cpu().numpy()\n",
    "        predicted_cpu = predicted.cpu().numpy()\n",
    "        f1 = f1_score(labels_cpu, predicted_cpu, average='macro')\n",
    "\n",
    "        return preds, loss, accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrixLogger(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preds = []\n",
    "        self.targets = []\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        ids = batch['ids']\n",
    "        mask = batch['mask']\n",
    "        labels = batch['labels']\n",
    "        preds = pl_module(ids, mask)\n",
    "        ground_truth_ids = labels.flatten().cpu().numpy()\n",
    "\n",
    "        self.preds.extend(preds.cpu().numpy())\n",
    "        self.targets.extend(ground_truth_ids)\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        all_labels = ['harm', 'question', 'concern']\n",
    "\n",
    "        # Log the confusion matrix\n",
    "        wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "            preds=np.argmax(np.array(self.preds), axis=1), y_true=self.targets, class_names=all_labels)\n",
    "        })\n",
    "\n",
    "        # Log the ROC curve\n",
    "        probabilites = torch.nn.functional.softmax(torch.tensor(self.preds), dim=1)\n",
    "        print(probabilites)\n",
    "        wandb.log({\"roc\" : wandb.plot.roc_curve(\n",
    "            y_true=self.targets, y_probas=probabilites, labels=all_labels, classes_to_plot=None)\n",
    "        })\n",
    "\n",
    "        # Clear for the next epoch\n",
    "        self.preds = []\n",
    "        self.targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_bert(run_config, train_dataloader, val_dataloader):\n",
    "    # WandB initialization \n",
    "    wandb.login()\n",
    "\n",
    "    # Initialize model\n",
    "    model = BERTClassifier(hparams=run_config)\n",
    "\n",
    "    # Initialize WandbLogger\n",
    "    wandb_logger = WandbLogger(entity='yvokeller', project='data-chatbot') # log_model='all'\n",
    "    wandb_logger.experiment.config.update(run_config)\n",
    "\n",
    "    # Create an instance of the ConfusionMatrixLogger class\n",
    "    confusion_matrix_logger = ConfusionMatrixLogger()\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=run_config.get('epochs'), \n",
    "        logger=wandb_logger,\n",
    "        callbacks=[confusion_matrix_logger],\n",
    "        log_every_n_steps=1, \n",
    "        enable_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "    # Close WandB logger\n",
    "    wandb.finish()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune BERT Model\n",
    "\n",
    "### POC with Simple Demo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader | No Batch: False; Batch Size: 32\n",
      "DataLoader | No Batch: True; Batch Size: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:398: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | BertForSequenceClassification | 177 M \n",
      "--------------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.423   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e587ff601f14369bc2f57d536651ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8e90d848e84885a3ff0869726c1705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a1eab01b864a4388e12d43479aaef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389bc67ab942446995fec47eccc2a2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0930fbfb0541a9b9c6eb6b324369c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b0b78ff58b410bac4137647367f450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5276561d30474e32be28267f830a866d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32db4a8e86fc46d5bde422f50205ea78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▃▃▃▅▅▅▆▆▆███</td></tr><tr><td>train/accuracy_epoch</td><td>██▁▁▁</td></tr><tr><td>train/accuracy_step</td><td>██▁▁▁</td></tr><tr><td>train/f1_epoch</td><td>██▁▁▁</td></tr><tr><td>train/f1_step</td><td>██▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▃▃▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▆██████</td></tr><tr><td>val/accuracy_epoch</td><td>▁▁▁▁▁</td></tr><tr><td>val/accuracy_step</td><td>▁▁▁▁▁</td></tr><tr><td>val/f1_epoch</td><td>▁▁▁▁▁</td></tr><tr><td>val/f1_step</td><td>▁▁▁▁▁</td></tr><tr><td>val/loss</td><td>█▇▆▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/accuracy_epoch</td><td>0.4</td></tr><tr><td>train/accuracy_step</td><td>0.4</td></tr><tr><td>train/f1_epoch</td><td>0.4</td></tr><tr><td>train/f1_step</td><td>0.4</td></tr><tr><td>train/loss</td><td>0.92182</td></tr><tr><td>trainer/global_step</td><td>4</td></tr><tr><td>val/accuracy_epoch</td><td>0.33333</td></tr><tr><td>val/accuracy_step</td><td>0.33333</td></tr><tr><td>val/f1_epoch</td><td>0.33333</td></tr><tr><td>val/f1_step</td><td>0.33333</td></tr><tr><td>val/loss</td><td>1.09284</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-rain-41</strong> at: <a href='https://wandb.ai/yvokeller/data-chatbot/runs/axwyha3j' target=\"_blank\">https://wandb.ai/yvokeller/data-chatbot/runs/axwyha3j</a><br/>Synced 6 W&B file(s), 12 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_144512-axwyha3j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WandB initialization \n",
    "wandb.login()\n",
    "\n",
    "# Config\n",
    "run_config = {\n",
    "    'epochs': 5,\n",
    "    'max_len': 100,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Load training and validation data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "train_dataloader = get_dataloader(train_df, tokenizer, max_len=run_config.get('max_len'), batch_size=run_config.get('batch_size'))\n",
    "\n",
    "val_df = pd.read_csv('data/val.csv')\n",
    "val_dataloader = get_dataloader(val_df, tokenizer, run_config.get('max_len'), batch_size=run_config.get('batch_size'), shuffle=False, nobatch=True)\n",
    "\n",
    "# Finetune BERT\n",
    "model = finetune_bert(run_config, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit on a small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (64, 2)\n",
      "DataLoader | No Batch: False; Batch Size: 16\n",
      "val (64, 2)\n",
      "DataLoader | No Batch: True; Batch Size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231026_171031-dchken98</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yvokeller/data-chatbot/runs/dchken98' target=\"_blank\">lucky-water-48</a></strong> to <a href='https://wandb.ai/yvokeller/data-chatbot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yvokeller/data-chatbot' target=\"_blank\">https://wandb.ai/yvokeller/data-chatbot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yvokeller/data-chatbot/runs/dchken98' target=\"_blank\">https://wandb.ai/yvokeller/data-chatbot/runs/dchken98</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | BertForSequenceClassification | 177 M \n",
      "--------------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.423   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14da97b9dca945cf8006f208307e744f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2829, 0.4150, 0.3021],\n",
      "        [0.2615, 0.4346, 0.3039],\n",
      "        [0.2804, 0.4186, 0.3010],\n",
      "        [0.2493, 0.4691, 0.2816],\n",
      "        [0.2803, 0.4108, 0.3089],\n",
      "        [0.2742, 0.4362, 0.2896],\n",
      "        [0.2442, 0.4593, 0.2965],\n",
      "        [0.2814, 0.4083, 0.3103],\n",
      "        [0.2695, 0.4215, 0.3089],\n",
      "        [0.2784, 0.4222, 0.2994],\n",
      "        [0.2697, 0.4309, 0.2994],\n",
      "        [0.2644, 0.4446, 0.2910],\n",
      "        [0.2559, 0.4373, 0.3069],\n",
      "        [0.2569, 0.4530, 0.2901],\n",
      "        [0.2545, 0.4522, 0.2932],\n",
      "        [0.2775, 0.4073, 0.3152],\n",
      "        [0.2669, 0.4440, 0.2891],\n",
      "        [0.2664, 0.4324, 0.3012],\n",
      "        [0.2858, 0.4220, 0.2923],\n",
      "        [0.2734, 0.4374, 0.2892],\n",
      "        [0.2737, 0.4481, 0.2782],\n",
      "        [0.2687, 0.4430, 0.2883],\n",
      "        [0.2737, 0.4104, 0.3159],\n",
      "        [0.2857, 0.4193, 0.2950],\n",
      "        [0.2696, 0.4331, 0.2974],\n",
      "        [0.2783, 0.4325, 0.2891],\n",
      "        [0.2581, 0.4601, 0.2818],\n",
      "        [0.2756, 0.4235, 0.3009],\n",
      "        [0.2600, 0.4397, 0.3002],\n",
      "        [0.2576, 0.4646, 0.2778],\n",
      "        [0.2691, 0.4450, 0.2859],\n",
      "        [0.2647, 0.4236, 0.3117],\n",
      "        [0.2679, 0.4031, 0.3290],\n",
      "        [0.2643, 0.4318, 0.3039],\n",
      "        [0.2564, 0.4629, 0.2807],\n",
      "        [0.2793, 0.4569, 0.2638],\n",
      "        [0.2744, 0.4140, 0.3116],\n",
      "        [0.2769, 0.4301, 0.2930],\n",
      "        [0.2651, 0.4307, 0.3041],\n",
      "        [0.3017, 0.4086, 0.2898],\n",
      "        [0.2737, 0.4620, 0.2643],\n",
      "        [0.2690, 0.4454, 0.2856],\n",
      "        [0.2706, 0.4281, 0.3013],\n",
      "        [0.2640, 0.4201, 0.3160],\n",
      "        [0.2619, 0.4543, 0.2838],\n",
      "        [0.2687, 0.4260, 0.3053],\n",
      "        [0.2687, 0.4035, 0.3278],\n",
      "        [0.2728, 0.4240, 0.3033],\n",
      "        [0.2882, 0.4179, 0.2939],\n",
      "        [0.2656, 0.4437, 0.2907],\n",
      "        [0.2932, 0.4039, 0.3029],\n",
      "        [0.2539, 0.4671, 0.2790],\n",
      "        [0.2661, 0.4190, 0.3149],\n",
      "        [0.2843, 0.4175, 0.2982],\n",
      "        [0.2819, 0.4378, 0.2803],\n",
      "        [0.2704, 0.4189, 0.3107],\n",
      "        [0.2771, 0.4206, 0.3023],\n",
      "        [0.3032, 0.3981, 0.2987],\n",
      "        [0.2985, 0.4081, 0.2934],\n",
      "        [0.2566, 0.4533, 0.2901],\n",
      "        [0.2706, 0.4381, 0.2913],\n",
      "        [0.2608, 0.4219, 0.3172],\n",
      "        [0.2740, 0.4193, 0.3067],\n",
      "        [0.2783, 0.4437, 0.2780]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe20b1a27b043378bf153911f0fcfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5401e300b5d4d498d17d686f6ac2d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3421, 0.3239, 0.3340],\n",
      "        [0.2976, 0.2749, 0.4275],\n",
      "        [0.2625, 0.4304, 0.3071],\n",
      "        [0.2740, 0.4196, 0.3064],\n",
      "        [0.3103, 0.3158, 0.3739],\n",
      "        [0.3204, 0.2866, 0.3930],\n",
      "        [0.2617, 0.3770, 0.3613],\n",
      "        [0.2382, 0.4637, 0.2981],\n",
      "        [0.2890, 0.3381, 0.3728],\n",
      "        [0.3136, 0.3413, 0.3450],\n",
      "        [0.2953, 0.3777, 0.3271],\n",
      "        [0.2099, 0.5159, 0.2742],\n",
      "        [0.3051, 0.2612, 0.4337],\n",
      "        [0.2947, 0.3606, 0.3447],\n",
      "        [0.2644, 0.4154, 0.3202],\n",
      "        [0.2425, 0.4438, 0.3137],\n",
      "        [0.2277, 0.4841, 0.2882],\n",
      "        [0.2812, 0.3162, 0.4026],\n",
      "        [0.2747, 0.4133, 0.3120],\n",
      "        [0.2781, 0.3506, 0.3713],\n",
      "        [0.2932, 0.3714, 0.3354],\n",
      "        [0.3175, 0.2840, 0.3985],\n",
      "        [0.2964, 0.3042, 0.3994],\n",
      "        [0.3530, 0.3249, 0.3221],\n",
      "        [0.3088, 0.2685, 0.4227],\n",
      "        [0.2991, 0.2980, 0.4029],\n",
      "        [0.2094, 0.5312, 0.2594],\n",
      "        [0.3086, 0.3275, 0.3639],\n",
      "        [0.2101, 0.5077, 0.2822],\n",
      "        [0.2329, 0.4918, 0.2753],\n",
      "        [0.2277, 0.4976, 0.2747],\n",
      "        [0.3076, 0.3394, 0.3530],\n",
      "        [0.2772, 0.3017, 0.4211],\n",
      "        [0.2855, 0.3269, 0.3876],\n",
      "        [0.2854, 0.3965, 0.3181],\n",
      "        [0.3193, 0.2829, 0.3977],\n",
      "        [0.2230, 0.4889, 0.2880],\n",
      "        [0.2419, 0.4872, 0.2709],\n",
      "        [0.2317, 0.4766, 0.2917],\n",
      "        [0.2650, 0.4570, 0.2780],\n",
      "        [0.3383, 0.2466, 0.4150],\n",
      "        [0.3085, 0.3616, 0.3299],\n",
      "        [0.2950, 0.2922, 0.4128],\n",
      "        [0.3017, 0.3388, 0.3595],\n",
      "        [0.2239, 0.5032, 0.2729],\n",
      "        [0.2388, 0.4530, 0.3083],\n",
      "        [0.2905, 0.3590, 0.3506],\n",
      "        [0.3148, 0.3417, 0.3435],\n",
      "        [0.2472, 0.4717, 0.2811],\n",
      "        [0.3079, 0.3438, 0.3483],\n",
      "        [0.3355, 0.3164, 0.3481],\n",
      "        [0.2900, 0.3916, 0.3184],\n",
      "        [0.2899, 0.3359, 0.3741],\n",
      "        [0.2291, 0.4998, 0.2711],\n",
      "        [0.3112, 0.3372, 0.3516],\n",
      "        [0.2412, 0.4585, 0.3003],\n",
      "        [0.2388, 0.4453, 0.3159],\n",
      "        [0.3951, 0.2590, 0.3459],\n",
      "        [0.2752, 0.4306, 0.2942],\n",
      "        [0.2186, 0.5043, 0.2771],\n",
      "        [0.3212, 0.3536, 0.3252],\n",
      "        [0.2747, 0.3184, 0.4069],\n",
      "        [0.2970, 0.3213, 0.3817],\n",
      "        [0.3641, 0.2301, 0.4058]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f643fd8e5b4a2bbb83fae3f0bde1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4665, 0.2218, 0.3116],\n",
      "        [0.3802, 0.1629, 0.4569],\n",
      "        [0.2383, 0.4972, 0.2645],\n",
      "        [0.3539, 0.3349, 0.3112],\n",
      "        [0.3989, 0.1937, 0.4074],\n",
      "        [0.4103, 0.1784, 0.4114],\n",
      "        [0.3026, 0.2805, 0.4170],\n",
      "        [0.2014, 0.5667, 0.2319],\n",
      "        [0.4566, 0.1407, 0.4027],\n",
      "        [0.4468, 0.2231, 0.3300],\n",
      "        [0.3798, 0.3226, 0.2977],\n",
      "        [0.1867, 0.5800, 0.2333],\n",
      "        [0.4398, 0.1290, 0.4312],\n",
      "        [0.4581, 0.1948, 0.3471],\n",
      "        [0.3038, 0.3821, 0.3141],\n",
      "        [0.2202, 0.5114, 0.2684],\n",
      "        [0.1935, 0.5696, 0.2369],\n",
      "        [0.3579, 0.1922, 0.4499],\n",
      "        [0.2535, 0.4800, 0.2665],\n",
      "        [0.3205, 0.2549, 0.4246],\n",
      "        [0.3497, 0.2921, 0.3582],\n",
      "        [0.4525, 0.1375, 0.4100],\n",
      "        [0.3813, 0.2001, 0.4186],\n",
      "        [0.4840, 0.2286, 0.2874],\n",
      "        [0.4367, 0.1374, 0.4259],\n",
      "        [0.4002, 0.1599, 0.4399],\n",
      "        [0.1859, 0.5953, 0.2188],\n",
      "        [0.3863, 0.2146, 0.3991],\n",
      "        [0.1816, 0.5929, 0.2255],\n",
      "        [0.2003, 0.5714, 0.2283],\n",
      "        [0.1916, 0.5914, 0.2170],\n",
      "        [0.4597, 0.1956, 0.3447],\n",
      "        [0.3301, 0.2097, 0.4602],\n",
      "        [0.4207, 0.1552, 0.4241],\n",
      "        [0.3491, 0.3314, 0.3195],\n",
      "        [0.4192, 0.1690, 0.4117],\n",
      "        [0.2029, 0.5596, 0.2375],\n",
      "        [0.2001, 0.5907, 0.2092],\n",
      "        [0.2021, 0.5643, 0.2336],\n",
      "        [0.2195, 0.5554, 0.2251],\n",
      "        [0.4561, 0.1305, 0.4134],\n",
      "        [0.4378, 0.2401, 0.3221],\n",
      "        [0.3719, 0.1968, 0.4313],\n",
      "        [0.4362, 0.2041, 0.3597],\n",
      "        [0.1957, 0.5752, 0.2291],\n",
      "        [0.2193, 0.5171, 0.2635],\n",
      "        [0.3573, 0.2890, 0.3537],\n",
      "        [0.4295, 0.2383, 0.3323],\n",
      "        [0.2073, 0.5689, 0.2238],\n",
      "        [0.4125, 0.2487, 0.3388],\n",
      "        [0.4662, 0.2191, 0.3147],\n",
      "        [0.3791, 0.3031, 0.3178],\n",
      "        [0.3284, 0.2533, 0.4183],\n",
      "        [0.1979, 0.5799, 0.2222],\n",
      "        [0.3865, 0.2154, 0.3981],\n",
      "        [0.2048, 0.5656, 0.2296],\n",
      "        [0.2169, 0.5193, 0.2639],\n",
      "        [0.4687, 0.1985, 0.3328],\n",
      "        [0.2428, 0.5172, 0.2400],\n",
      "        [0.1921, 0.5823, 0.2256],\n",
      "        [0.4269, 0.2719, 0.3011],\n",
      "        [0.3261, 0.2221, 0.4518],\n",
      "        [0.3879, 0.1980, 0.4142],\n",
      "        [0.4639, 0.1326, 0.4035]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156a0381e97d41baa6814d03059b9ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5353, 0.1706, 0.2940],\n",
      "        [0.3918, 0.1237, 0.4845],\n",
      "        [0.1881, 0.6110, 0.2009],\n",
      "        [0.4364, 0.2654, 0.2982],\n",
      "        [0.4589, 0.1321, 0.4090],\n",
      "        [0.4111, 0.1336, 0.4554],\n",
      "        [0.2969, 0.2444, 0.4587],\n",
      "        [0.1648, 0.6522, 0.1829],\n",
      "        [0.4052, 0.2185, 0.3762],\n",
      "        [0.5357, 0.1747, 0.2896],\n",
      "        [0.4287, 0.3262, 0.2451],\n",
      "        [0.1549, 0.6613, 0.1838],\n",
      "        [0.4585, 0.1128, 0.4288],\n",
      "        [0.4564, 0.2226, 0.3210],\n",
      "        [0.3405, 0.3501, 0.3095],\n",
      "        [0.1784, 0.6180, 0.2036],\n",
      "        [0.1591, 0.6564, 0.1845],\n",
      "        [0.3628, 0.1333, 0.5039],\n",
      "        [0.2130, 0.5715, 0.2154],\n",
      "        [0.3746, 0.1554, 0.4700],\n",
      "        [0.4480, 0.1785, 0.3735],\n",
      "        [0.4092, 0.1399, 0.4509],\n",
      "        [0.3833, 0.1512, 0.4655],\n",
      "        [0.5731, 0.1796, 0.2473],\n",
      "        [0.4528, 0.1175, 0.4297],\n",
      "        [0.4234, 0.1238, 0.4528],\n",
      "        [0.1548, 0.6676, 0.1776],\n",
      "        [0.4603, 0.1374, 0.4023],\n",
      "        [0.1511, 0.6727, 0.1762],\n",
      "        [0.1622, 0.6589, 0.1789],\n",
      "        [0.1598, 0.6659, 0.1742],\n",
      "        [0.5040, 0.1493, 0.3467],\n",
      "        [0.3271, 0.1617, 0.5112],\n",
      "        [0.4549, 0.1206, 0.4245],\n",
      "        [0.3725, 0.3352, 0.2923],\n",
      "        [0.4194, 0.1244, 0.4561],\n",
      "        [0.1635, 0.6588, 0.1777],\n",
      "        [0.1649, 0.6735, 0.1616],\n",
      "        [0.1660, 0.6497, 0.1843],\n",
      "        [0.1752, 0.6449, 0.1799],\n",
      "        [0.4618, 0.1167, 0.4215],\n",
      "        [0.5148, 0.1740, 0.3111],\n",
      "        [0.3760, 0.1482, 0.4759],\n",
      "        [0.4774, 0.1615, 0.3611],\n",
      "        [0.1575, 0.6651, 0.1774],\n",
      "        [0.1822, 0.6136, 0.2042],\n",
      "        [0.4609, 0.2120, 0.3271],\n",
      "        [0.5086, 0.1838, 0.3076],\n",
      "        [0.1701, 0.6535, 0.1765],\n",
      "        [0.4780, 0.1858, 0.3362],\n",
      "        [0.5263, 0.1920, 0.2818],\n",
      "        [0.4590, 0.2467, 0.2943],\n",
      "        [0.3865, 0.1745, 0.4390],\n",
      "        [0.1631, 0.6633, 0.1736],\n",
      "        [0.4675, 0.1324, 0.4001],\n",
      "        [0.1724, 0.6469, 0.1807],\n",
      "        [0.1761, 0.6209, 0.2030],\n",
      "        [0.5323, 0.1599, 0.3079],\n",
      "        [0.1960, 0.6216, 0.1824],\n",
      "        [0.1609, 0.6593, 0.1798],\n",
      "        [0.5095, 0.2337, 0.2568],\n",
      "        [0.3385, 0.1585, 0.5030],\n",
      "        [0.4515, 0.1293, 0.4191],\n",
      "        [0.4683, 0.1170, 0.4147]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce27c0247b8847a4a491833e3c64b367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Config\n",
    "run_config = {\n",
    "    'epochs': 10,\n",
    "    'max_len': None,\n",
    "    'batch_size': 16\n",
    "}\n",
    "\n",
    "# Load training and validation data\n",
    "train_df = pd.read_parquet('../data/train.parquet')\n",
    "train_df, _ = train_test_split(train_df, train_size=64, random_state=42, stratify=train_df['label'])\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "print('train', train_df.shape)\n",
    "train_dataloader = get_dataloader(train_df, tokenizer, max_len=run_config.get('max_len'), batch_size=run_config.get('batch_size'))\n",
    "\n",
    "val_df = pd.read_parquet('../data/test.parquet')\n",
    "val_df, _ = train_test_split(val_df, train_size=64, random_state=42, stratify=val_df['label'])\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "print('val', val_df.shape)\n",
    "val_dataloader = get_dataloader(val_df, tokenizer, run_config.get('max_len'), batch_size=run_config.get('batch_size'), shuffle=False, nobatch=True)\n",
    "\n",
    "# Fine-tune BERT\n",
    "model = finetune_bert(run_config, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (1074, 2)\n",
      "DataLoader | No Batch: False; Batch Size: 32\n",
      "val (264, 2)\n",
      "DataLoader | No Batch: True; Batch Size: 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231026_170340-2t5q4vb4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yvokeller/data-chatbot/runs/2t5q4vb4' target=\"_blank\">ethereal-water-47</a></strong> to <a href='https://wandb.ai/yvokeller/data-chatbot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yvokeller/data-chatbot' target=\"_blank\">https://wandb.ai/yvokeller/data-chatbot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yvokeller/data-chatbot/runs/2t5q4vb4' target=\"_blank\">https://wandb.ai/yvokeller/data-chatbot/runs/2t5q4vb4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | BertForSequenceClassification | 177 M \n",
      "--------------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.423   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2355c1f57fcc415c9f5a7c3b0fd36c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2754, 0.3049, 0.4197],\n",
      "        [0.2776, 0.3188, 0.4036],\n",
      "        [0.2711, 0.3209, 0.4079],\n",
      "        [0.2778, 0.3133, 0.4089],\n",
      "        [0.2741, 0.3072, 0.4186],\n",
      "        [0.2453, 0.3437, 0.4109],\n",
      "        [0.2589, 0.3094, 0.4317],\n",
      "        [0.2679, 0.3197, 0.4124],\n",
      "        [0.2659, 0.3073, 0.4267],\n",
      "        [0.2669, 0.3281, 0.4050],\n",
      "        [0.2657, 0.3050, 0.4293],\n",
      "        [0.2763, 0.3103, 0.4133],\n",
      "        [0.2561, 0.3140, 0.4299],\n",
      "        [0.2600, 0.3124, 0.4276],\n",
      "        [0.2828, 0.3166, 0.4006],\n",
      "        [0.2676, 0.3259, 0.4065],\n",
      "        [0.2492, 0.3166, 0.4342],\n",
      "        [0.2571, 0.3171, 0.4259],\n",
      "        [0.2411, 0.3000, 0.4589],\n",
      "        [0.2370, 0.3216, 0.4414],\n",
      "        [0.2672, 0.3276, 0.4052],\n",
      "        [0.2333, 0.3315, 0.4352],\n",
      "        [0.2557, 0.3224, 0.4219],\n",
      "        [0.2410, 0.3262, 0.4329],\n",
      "        [0.2856, 0.2990, 0.4154],\n",
      "        [0.2499, 0.3166, 0.4335],\n",
      "        [0.2840, 0.3283, 0.3877],\n",
      "        [0.2816, 0.3011, 0.4172],\n",
      "        [0.2552, 0.3155, 0.4293],\n",
      "        [0.2841, 0.3162, 0.3998],\n",
      "        [0.2781, 0.3096, 0.4123],\n",
      "        [0.2518, 0.3213, 0.4269],\n",
      "        [0.2866, 0.3120, 0.4014],\n",
      "        [0.2519, 0.3302, 0.4178],\n",
      "        [0.2542, 0.3192, 0.4265],\n",
      "        [0.2548, 0.3143, 0.4309],\n",
      "        [0.2529, 0.3272, 0.4198],\n",
      "        [0.2578, 0.3203, 0.4218],\n",
      "        [0.2586, 0.3257, 0.4157],\n",
      "        [0.2513, 0.3174, 0.4313],\n",
      "        [0.2566, 0.3227, 0.4207],\n",
      "        [0.2516, 0.3154, 0.4329],\n",
      "        [0.2832, 0.3142, 0.4026],\n",
      "        [0.2470, 0.3224, 0.4306],\n",
      "        [0.2682, 0.3091, 0.4228],\n",
      "        [0.2651, 0.3177, 0.4172],\n",
      "        [0.2658, 0.3064, 0.4278],\n",
      "        [0.2710, 0.3125, 0.4165],\n",
      "        [0.2642, 0.3207, 0.4151],\n",
      "        [0.2539, 0.3130, 0.4331],\n",
      "        [0.2753, 0.3039, 0.4208],\n",
      "        [0.2527, 0.3205, 0.4268],\n",
      "        [0.2322, 0.3284, 0.4395],\n",
      "        [0.2366, 0.3237, 0.4397],\n",
      "        [0.2650, 0.3251, 0.4099],\n",
      "        [0.2494, 0.3190, 0.4316],\n",
      "        [0.2490, 0.3144, 0.4366],\n",
      "        [0.2858, 0.3095, 0.4047],\n",
      "        [0.2431, 0.3178, 0.4391],\n",
      "        [0.2862, 0.3130, 0.4008],\n",
      "        [0.2852, 0.3278, 0.3870],\n",
      "        [0.2747, 0.3234, 0.4019],\n",
      "        [0.2764, 0.3133, 0.4103],\n",
      "        [0.2934, 0.3109, 0.3957],\n",
      "        [0.2999, 0.3092, 0.3909],\n",
      "        [0.2670, 0.3089, 0.4240],\n",
      "        [0.2578, 0.3223, 0.4199],\n",
      "        [0.2854, 0.3136, 0.4011],\n",
      "        [0.2483, 0.3260, 0.4256],\n",
      "        [0.2545, 0.3110, 0.4345],\n",
      "        [0.2714, 0.3066, 0.4220],\n",
      "        [0.2588, 0.3141, 0.4270],\n",
      "        [0.2517, 0.3181, 0.4302],\n",
      "        [0.2586, 0.3106, 0.4308],\n",
      "        [0.2472, 0.3249, 0.4278],\n",
      "        [0.2732, 0.3147, 0.4121],\n",
      "        [0.2842, 0.3145, 0.4013],\n",
      "        [0.2364, 0.3183, 0.4453],\n",
      "        [0.2676, 0.3124, 0.4200],\n",
      "        [0.2518, 0.3292, 0.4190],\n",
      "        [0.2349, 0.3355, 0.4296],\n",
      "        [0.2890, 0.3201, 0.3909],\n",
      "        [0.2723, 0.3083, 0.4194],\n",
      "        [0.2608, 0.3094, 0.4297],\n",
      "        [0.2626, 0.3035, 0.4339],\n",
      "        [0.2458, 0.3252, 0.4290],\n",
      "        [0.2632, 0.3144, 0.4224],\n",
      "        [0.2588, 0.3213, 0.4199],\n",
      "        [0.2802, 0.3206, 0.3991],\n",
      "        [0.2567, 0.3432, 0.4001],\n",
      "        [0.2632, 0.3098, 0.4270],\n",
      "        [0.2649, 0.3107, 0.4244],\n",
      "        [0.2699, 0.3134, 0.4168],\n",
      "        [0.2606, 0.3253, 0.4141],\n",
      "        [0.2402, 0.3149, 0.4449],\n",
      "        [0.3034, 0.3110, 0.3856],\n",
      "        [0.2443, 0.3345, 0.4212],\n",
      "        [0.2783, 0.3239, 0.3977],\n",
      "        [0.2768, 0.3115, 0.4117],\n",
      "        [0.2661, 0.3240, 0.4099],\n",
      "        [0.2762, 0.3235, 0.4003],\n",
      "        [0.2741, 0.3202, 0.4057],\n",
      "        [0.2556, 0.3084, 0.4360],\n",
      "        [0.2737, 0.3097, 0.4166],\n",
      "        [0.2724, 0.3226, 0.4050],\n",
      "        [0.2689, 0.3135, 0.4176],\n",
      "        [0.2472, 0.3179, 0.4350],\n",
      "        [0.2603, 0.3136, 0.4261],\n",
      "        [0.2894, 0.3058, 0.4048],\n",
      "        [0.2685, 0.3103, 0.4212],\n",
      "        [0.2596, 0.3476, 0.3928],\n",
      "        [0.2649, 0.3158, 0.4193],\n",
      "        [0.2764, 0.3163, 0.4073],\n",
      "        [0.2520, 0.3140, 0.4340],\n",
      "        [0.2813, 0.3131, 0.4056],\n",
      "        [0.2608, 0.3239, 0.4153],\n",
      "        [0.2756, 0.2981, 0.4263],\n",
      "        [0.2648, 0.3249, 0.4102],\n",
      "        [0.2629, 0.3229, 0.4142],\n",
      "        [0.2505, 0.3431, 0.4064],\n",
      "        [0.2676, 0.3040, 0.4284],\n",
      "        [0.2369, 0.3193, 0.4438],\n",
      "        [0.2725, 0.3211, 0.4064],\n",
      "        [0.2633, 0.3013, 0.4353],\n",
      "        [0.2685, 0.3159, 0.4155],\n",
      "        [0.2470, 0.3140, 0.4389],\n",
      "        [0.2596, 0.3123, 0.4281],\n",
      "        [0.2402, 0.3249, 0.4350],\n",
      "        [0.2743, 0.3086, 0.4172],\n",
      "        [0.2507, 0.3137, 0.4356],\n",
      "        [0.2519, 0.3230, 0.4252],\n",
      "        [0.2571, 0.3168, 0.4261],\n",
      "        [0.2659, 0.3078, 0.4263],\n",
      "        [0.2561, 0.3135, 0.4304],\n",
      "        [0.2335, 0.3204, 0.4461],\n",
      "        [0.2424, 0.3307, 0.4269],\n",
      "        [0.2721, 0.3136, 0.4143],\n",
      "        [0.2676, 0.3057, 0.4267],\n",
      "        [0.2648, 0.3222, 0.4130],\n",
      "        [0.2707, 0.3202, 0.4091],\n",
      "        [0.2720, 0.3115, 0.4166],\n",
      "        [0.2712, 0.3232, 0.4056],\n",
      "        [0.2507, 0.3159, 0.4334],\n",
      "        [0.2579, 0.3275, 0.4146],\n",
      "        [0.2630, 0.3131, 0.4239],\n",
      "        [0.2512, 0.3173, 0.4315],\n",
      "        [0.2436, 0.3137, 0.4427],\n",
      "        [0.2780, 0.3093, 0.4127],\n",
      "        [0.2516, 0.3312, 0.4171],\n",
      "        [0.2896, 0.3180, 0.3924],\n",
      "        [0.2491, 0.3351, 0.4158],\n",
      "        [0.2688, 0.3361, 0.3952],\n",
      "        [0.2676, 0.3089, 0.4235],\n",
      "        [0.2485, 0.3206, 0.4309],\n",
      "        [0.2870, 0.3161, 0.3969],\n",
      "        [0.2798, 0.3028, 0.4173],\n",
      "        [0.2708, 0.3269, 0.4023],\n",
      "        [0.2525, 0.3332, 0.4143],\n",
      "        [0.2362, 0.3171, 0.4467],\n",
      "        [0.2671, 0.3075, 0.4254],\n",
      "        [0.2589, 0.3447, 0.3964],\n",
      "        [0.2653, 0.3140, 0.4207],\n",
      "        [0.2512, 0.3308, 0.4179],\n",
      "        [0.2651, 0.3090, 0.4259],\n",
      "        [0.2445, 0.3062, 0.4493],\n",
      "        [0.2478, 0.3101, 0.4422],\n",
      "        [0.2588, 0.3078, 0.4335],\n",
      "        [0.2817, 0.3171, 0.4011],\n",
      "        [0.2816, 0.3114, 0.4070],\n",
      "        [0.2559, 0.3403, 0.4038],\n",
      "        [0.2615, 0.3072, 0.4313],\n",
      "        [0.2497, 0.3270, 0.4233],\n",
      "        [0.2451, 0.3055, 0.4494],\n",
      "        [0.2506, 0.3057, 0.4436],\n",
      "        [0.2444, 0.3282, 0.4274],\n",
      "        [0.2644, 0.3117, 0.4239],\n",
      "        [0.2700, 0.3077, 0.4223],\n",
      "        [0.2594, 0.3281, 0.4125],\n",
      "        [0.2715, 0.3197, 0.4088],\n",
      "        [0.2823, 0.2934, 0.4242],\n",
      "        [0.2616, 0.3205, 0.4179],\n",
      "        [0.2634, 0.3392, 0.3973],\n",
      "        [0.2329, 0.3212, 0.4460],\n",
      "        [0.2614, 0.3147, 0.4239],\n",
      "        [0.2623, 0.3146, 0.4231],\n",
      "        [0.2930, 0.3013, 0.4057],\n",
      "        [0.2788, 0.3278, 0.3935],\n",
      "        [0.2771, 0.3193, 0.4036],\n",
      "        [0.2428, 0.3204, 0.4368],\n",
      "        [0.2628, 0.3229, 0.4143],\n",
      "        [0.2848, 0.2996, 0.4156],\n",
      "        [0.2806, 0.3190, 0.4003],\n",
      "        [0.2577, 0.3006, 0.4417],\n",
      "        [0.2816, 0.3108, 0.4076],\n",
      "        [0.2923, 0.3237, 0.3839],\n",
      "        [0.2740, 0.3059, 0.4201],\n",
      "        [0.2810, 0.3091, 0.4099],\n",
      "        [0.2547, 0.3126, 0.4327],\n",
      "        [0.2572, 0.3155, 0.4273],\n",
      "        [0.2741, 0.3025, 0.4234],\n",
      "        [0.2553, 0.3116, 0.4330],\n",
      "        [0.2714, 0.3033, 0.4254],\n",
      "        [0.3036, 0.3031, 0.3933],\n",
      "        [0.3008, 0.3074, 0.3917],\n",
      "        [0.2786, 0.3093, 0.4122],\n",
      "        [0.2885, 0.3056, 0.4060],\n",
      "        [0.2602, 0.3157, 0.4241],\n",
      "        [0.2387, 0.3151, 0.4461],\n",
      "        [0.2930, 0.2986, 0.4084],\n",
      "        [0.2609, 0.2977, 0.4414],\n",
      "        [0.2464, 0.3122, 0.4414],\n",
      "        [0.2994, 0.3009, 0.3997],\n",
      "        [0.2557, 0.3091, 0.4352],\n",
      "        [0.2994, 0.3019, 0.3987],\n",
      "        [0.2539, 0.3161, 0.4299],\n",
      "        [0.2541, 0.3261, 0.4198],\n",
      "        [0.2556, 0.3092, 0.4352],\n",
      "        [0.2508, 0.3206, 0.4286],\n",
      "        [0.2732, 0.3102, 0.4165],\n",
      "        [0.2746, 0.3059, 0.4195],\n",
      "        [0.2659, 0.3154, 0.4187],\n",
      "        [0.2791, 0.3139, 0.4070],\n",
      "        [0.2623, 0.3176, 0.4202],\n",
      "        [0.2817, 0.3164, 0.4019],\n",
      "        [0.2963, 0.3093, 0.3944],\n",
      "        [0.2608, 0.3171, 0.4221],\n",
      "        [0.2678, 0.3142, 0.4179],\n",
      "        [0.2445, 0.3102, 0.4453],\n",
      "        [0.2845, 0.3203, 0.3952],\n",
      "        [0.2294, 0.3432, 0.4274],\n",
      "        [0.2716, 0.3105, 0.4179],\n",
      "        [0.2277, 0.3229, 0.4494],\n",
      "        [0.2862, 0.3172, 0.3966],\n",
      "        [0.2633, 0.3168, 0.4199],\n",
      "        [0.2709, 0.3065, 0.4226],\n",
      "        [0.2658, 0.3117, 0.4225],\n",
      "        [0.2987, 0.3109, 0.3904],\n",
      "        [0.2605, 0.3061, 0.4334],\n",
      "        [0.2643, 0.3104, 0.4253],\n",
      "        [0.2765, 0.3147, 0.4088],\n",
      "        [0.2659, 0.3150, 0.4191],\n",
      "        [0.2753, 0.3119, 0.4128],\n",
      "        [0.2714, 0.3214, 0.4072],\n",
      "        [0.2916, 0.3210, 0.3874],\n",
      "        [0.2810, 0.3346, 0.3844],\n",
      "        [0.2644, 0.3091, 0.4265],\n",
      "        [0.2540, 0.3089, 0.4371],\n",
      "        [0.3045, 0.3061, 0.3894],\n",
      "        [0.2527, 0.3120, 0.4353],\n",
      "        [0.2692, 0.3174, 0.4134],\n",
      "        [0.2713, 0.3222, 0.4065],\n",
      "        [0.2557, 0.3231, 0.4212],\n",
      "        [0.2639, 0.3103, 0.4258],\n",
      "        [0.2505, 0.3189, 0.4306],\n",
      "        [0.2596, 0.3224, 0.4180],\n",
      "        [0.2641, 0.3161, 0.4198],\n",
      "        [0.2557, 0.3126, 0.4317],\n",
      "        [0.2437, 0.3081, 0.4482],\n",
      "        [0.2602, 0.3208, 0.4190],\n",
      "        [0.2788, 0.3115, 0.4096],\n",
      "        [0.2748, 0.3147, 0.4106],\n",
      "        [0.2699, 0.3161, 0.4140],\n",
      "        [0.2625, 0.3098, 0.4277],\n",
      "        [0.2737, 0.3063, 0.4200]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627256c40f9041a38882fae38bf47162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d2930220c2487681a2c1e8055c98b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0354, 0.9350, 0.0296],\n",
      "        [0.0322, 0.9369, 0.0309],\n",
      "        [0.0322, 0.9386, 0.0292],\n",
      "        [0.0338, 0.9331, 0.0331],\n",
      "        [0.0360, 0.9354, 0.0286],\n",
      "        [0.0314, 0.9361, 0.0326],\n",
      "        [0.0313, 0.9375, 0.0312],\n",
      "        [0.0339, 0.9355, 0.0306],\n",
      "        [0.0325, 0.9384, 0.0291],\n",
      "        [0.0341, 0.9341, 0.0318],\n",
      "        [0.0323, 0.9338, 0.0339],\n",
      "        [0.0340, 0.9340, 0.0320],\n",
      "        [0.0322, 0.9357, 0.0322],\n",
      "        [0.0321, 0.9361, 0.0318],\n",
      "        [0.0333, 0.9367, 0.0301],\n",
      "        [0.0360, 0.9367, 0.0273],\n",
      "        [0.0321, 0.9368, 0.0311],\n",
      "        [0.0359, 0.9352, 0.0289],\n",
      "        [0.0406, 0.9282, 0.0312],\n",
      "        [0.0318, 0.9357, 0.0325],\n",
      "        [0.0298, 0.9336, 0.0367],\n",
      "        [0.0327, 0.9332, 0.0341],\n",
      "        [0.0317, 0.9336, 0.0347],\n",
      "        [0.0320, 0.9375, 0.0305],\n",
      "        [0.0327, 0.9373, 0.0300],\n",
      "        [0.0317, 0.9372, 0.0311],\n",
      "        [0.0320, 0.9354, 0.0326],\n",
      "        [0.0330, 0.9370, 0.0300],\n",
      "        [0.0333, 0.9350, 0.0318],\n",
      "        [0.0320, 0.9382, 0.0298],\n",
      "        [0.0336, 0.9359, 0.0305],\n",
      "        [0.0324, 0.9359, 0.0317],\n",
      "        [0.0354, 0.9360, 0.0286],\n",
      "        [0.0306, 0.9376, 0.0318],\n",
      "        [0.0300, 0.9352, 0.0349],\n",
      "        [0.0354, 0.9348, 0.0298],\n",
      "        [0.0335, 0.9344, 0.0321],\n",
      "        [0.0327, 0.9348, 0.0325],\n",
      "        [0.0315, 0.9375, 0.0309],\n",
      "        [0.0334, 0.9352, 0.0314],\n",
      "        [0.0322, 0.9368, 0.0311],\n",
      "        [0.0332, 0.9355, 0.0313],\n",
      "        [0.0361, 0.9341, 0.0298],\n",
      "        [0.0292, 0.9353, 0.0355],\n",
      "        [0.0311, 0.9360, 0.0329],\n",
      "        [0.0306, 0.9348, 0.0346],\n",
      "        [0.0360, 0.9312, 0.0328],\n",
      "        [0.0358, 0.9344, 0.0297],\n",
      "        [0.0320, 0.9376, 0.0304],\n",
      "        [0.0324, 0.9363, 0.0313],\n",
      "        [0.0342, 0.9351, 0.0307],\n",
      "        [0.0346, 0.9360, 0.0294],\n",
      "        [0.0306, 0.9359, 0.0335],\n",
      "        [0.0330, 0.9370, 0.0300],\n",
      "        [0.0352, 0.9358, 0.0290],\n",
      "        [0.0346, 0.9362, 0.0292],\n",
      "        [0.0311, 0.9373, 0.0316],\n",
      "        [0.0328, 0.9377, 0.0296],\n",
      "        [0.0331, 0.9368, 0.0301],\n",
      "        [0.0351, 0.9344, 0.0305],\n",
      "        [0.0340, 0.9349, 0.0311],\n",
      "        [0.0331, 0.9337, 0.0332],\n",
      "        [0.0342, 0.9310, 0.0348],\n",
      "        [0.0312, 0.9355, 0.0332],\n",
      "        [0.0336, 0.9347, 0.0318],\n",
      "        [0.0316, 0.9347, 0.0338],\n",
      "        [0.0317, 0.9378, 0.0305],\n",
      "        [0.0302, 0.9373, 0.0326],\n",
      "        [0.0308, 0.9381, 0.0312],\n",
      "        [0.0330, 0.9362, 0.0308],\n",
      "        [0.0351, 0.9305, 0.0344],\n",
      "        [0.0356, 0.9347, 0.0296],\n",
      "        [0.0333, 0.9302, 0.0365],\n",
      "        [0.0344, 0.9358, 0.0298],\n",
      "        [0.0319, 0.9343, 0.0338],\n",
      "        [0.0332, 0.9348, 0.0320],\n",
      "        [0.0368, 0.9348, 0.0285],\n",
      "        [0.0323, 0.9359, 0.0318],\n",
      "        [0.0313, 0.9356, 0.0331],\n",
      "        [0.0358, 0.9326, 0.0316],\n",
      "        [0.0325, 0.9359, 0.0316],\n",
      "        [0.0328, 0.9342, 0.0330],\n",
      "        [0.0355, 0.9363, 0.0282],\n",
      "        [0.0346, 0.9362, 0.0292],\n",
      "        [0.0344, 0.9355, 0.0302],\n",
      "        [0.0339, 0.9361, 0.0300],\n",
      "        [0.0337, 0.9363, 0.0299],\n",
      "        [0.0319, 0.9363, 0.0318],\n",
      "        [0.7795, 0.0627, 0.1577],\n",
      "        [0.8901, 0.0490, 0.0609],\n",
      "        [0.8179, 0.0573, 0.1249],\n",
      "        [0.7340, 0.0405, 0.2256],\n",
      "        [0.8677, 0.0664, 0.0660],\n",
      "        [0.8650, 0.0562, 0.0788],\n",
      "        [0.7390, 0.0681, 0.1929],\n",
      "        [0.6321, 0.0730, 0.2950],\n",
      "        [0.8654, 0.0532, 0.0814],\n",
      "        [0.8705, 0.0608, 0.0687],\n",
      "        [0.4263, 0.0455, 0.5282],\n",
      "        [0.8735, 0.0590, 0.0675],\n",
      "        [0.8776, 0.0670, 0.0554],\n",
      "        [0.8814, 0.0558, 0.0628],\n",
      "        [0.8148, 0.0551, 0.1300],\n",
      "        [0.8711, 0.0503, 0.0786],\n",
      "        [0.8440, 0.0467, 0.1093],\n",
      "        [0.1436, 0.8178, 0.0386],\n",
      "        [0.8735, 0.0538, 0.0726],\n",
      "        [0.8657, 0.0817, 0.0527],\n",
      "        [0.8609, 0.0505, 0.0886],\n",
      "        [0.8416, 0.1050, 0.0534],\n",
      "        [0.8797, 0.0652, 0.0551],\n",
      "        [0.8791, 0.0574, 0.0635],\n",
      "        [0.8593, 0.0561, 0.0846],\n",
      "        [0.8840, 0.0555, 0.0605],\n",
      "        [0.7488, 0.0464, 0.2049],\n",
      "        [0.8838, 0.0506, 0.0656],\n",
      "        [0.8645, 0.0488, 0.0867],\n",
      "        [0.8422, 0.0604, 0.0974],\n",
      "        [0.8820, 0.0608, 0.0573],\n",
      "        [0.8782, 0.0553, 0.0665],\n",
      "        [0.8628, 0.0426, 0.0946],\n",
      "        [0.8261, 0.0525, 0.1214],\n",
      "        [0.0669, 0.8940, 0.0391],\n",
      "        [0.7594, 0.0472, 0.1934],\n",
      "        [0.8578, 0.0623, 0.0799],\n",
      "        [0.6355, 0.0465, 0.3180],\n",
      "        [0.8601, 0.0583, 0.0816],\n",
      "        [0.8796, 0.0529, 0.0675],\n",
      "        [0.8571, 0.0506, 0.0923],\n",
      "        [0.8684, 0.0630, 0.0685],\n",
      "        [0.8465, 0.0494, 0.1041],\n",
      "        [0.8365, 0.0921, 0.0714],\n",
      "        [0.8592, 0.0573, 0.0835],\n",
      "        [0.8775, 0.0584, 0.0641],\n",
      "        [0.7849, 0.0541, 0.1610],\n",
      "        [0.8798, 0.0555, 0.0647],\n",
      "        [0.3244, 0.0614, 0.6142],\n",
      "        [0.8721, 0.0535, 0.0745],\n",
      "        [0.8868, 0.0506, 0.0627],\n",
      "        [0.8610, 0.0569, 0.0821],\n",
      "        [0.8606, 0.0696, 0.0698],\n",
      "        [0.8802, 0.0523, 0.0675],\n",
      "        [0.8552, 0.0461, 0.0987],\n",
      "        [0.8555, 0.0468, 0.0978],\n",
      "        [0.8785, 0.0460, 0.0755],\n",
      "        [0.8700, 0.0583, 0.0717],\n",
      "        [0.8813, 0.0547, 0.0641],\n",
      "        [0.6955, 0.0741, 0.2304],\n",
      "        [0.8778, 0.0575, 0.0647],\n",
      "        [0.8329, 0.0667, 0.1005],\n",
      "        [0.8858, 0.0517, 0.0625],\n",
      "        [0.8784, 0.0518, 0.0698],\n",
      "        [0.8845, 0.0510, 0.0645],\n",
      "        [0.8696, 0.0634, 0.0671],\n",
      "        [0.8774, 0.0544, 0.0681],\n",
      "        [0.8413, 0.0730, 0.0857],\n",
      "        [0.8723, 0.0590, 0.0687],\n",
      "        [0.8673, 0.0556, 0.0771],\n",
      "        [0.8616, 0.0625, 0.0759],\n",
      "        [0.8610, 0.0632, 0.0758],\n",
      "        [0.8858, 0.0561, 0.0581],\n",
      "        [0.8559, 0.0524, 0.0918],\n",
      "        [0.8568, 0.0574, 0.0858],\n",
      "        [0.1497, 0.0363, 0.8139],\n",
      "        [0.5233, 0.0454, 0.4313],\n",
      "        [0.8540, 0.0584, 0.0877],\n",
      "        [0.8615, 0.0566, 0.0819],\n",
      "        [0.8490, 0.0704, 0.0806],\n",
      "        [0.8303, 0.0389, 0.1308],\n",
      "        [0.8590, 0.0561, 0.0849],\n",
      "        [0.8276, 0.0503, 0.1221],\n",
      "        [0.8667, 0.0696, 0.0637],\n",
      "        [0.8470, 0.0671, 0.0859],\n",
      "        [0.8582, 0.0652, 0.0765],\n",
      "        [0.6680, 0.0534, 0.2786],\n",
      "        [0.8623, 0.0542, 0.0835],\n",
      "        [0.0574, 0.0482, 0.8944],\n",
      "        [0.0528, 0.0485, 0.8987],\n",
      "        [0.1052, 0.0575, 0.8373],\n",
      "        [0.0589, 0.0570, 0.8841],\n",
      "        [0.0860, 0.0471, 0.8669],\n",
      "        [0.5331, 0.0481, 0.4188],\n",
      "        [0.0518, 0.0611, 0.8872],\n",
      "        [0.0541, 0.0532, 0.8928],\n",
      "        [0.0574, 0.0517, 0.8909],\n",
      "        [0.1016, 0.0396, 0.8588],\n",
      "        [0.0648, 0.0470, 0.8882],\n",
      "        [0.0550, 0.0493, 0.8957],\n",
      "        [0.0512, 0.0445, 0.9044],\n",
      "        [0.0576, 0.0549, 0.8875],\n",
      "        [0.0769, 0.0447, 0.8784],\n",
      "        [0.0564, 0.0465, 0.8971],\n",
      "        [0.0555, 0.0508, 0.8937],\n",
      "        [0.0806, 0.0413, 0.8781],\n",
      "        [0.0634, 0.0514, 0.8852],\n",
      "        [0.0550, 0.0473, 0.8977],\n",
      "        [0.0560, 0.0435, 0.9004],\n",
      "        [0.0548, 0.0519, 0.8934],\n",
      "        [0.0558, 0.0450, 0.8992],\n",
      "        [0.0575, 0.0494, 0.8931],\n",
      "        [0.2805, 0.1444, 0.5751],\n",
      "        [0.0558, 0.0486, 0.8956],\n",
      "        [0.0557, 0.0450, 0.8993],\n",
      "        [0.0569, 0.0474, 0.8957],\n",
      "        [0.0716, 0.0423, 0.8861],\n",
      "        [0.0532, 0.0535, 0.8933],\n",
      "        [0.0609, 0.0562, 0.8829],\n",
      "        [0.0537, 0.0505, 0.8958],\n",
      "        [0.0579, 0.0487, 0.8934],\n",
      "        [0.0607, 0.0467, 0.8926],\n",
      "        [0.0524, 0.0524, 0.8952],\n",
      "        [0.0587, 0.0448, 0.8965],\n",
      "        [0.1190, 0.0401, 0.8409],\n",
      "        [0.0551, 0.0439, 0.9010],\n",
      "        [0.0532, 0.0531, 0.8937],\n",
      "        [0.0552, 0.0489, 0.8959],\n",
      "        [0.0579, 0.0440, 0.8981],\n",
      "        [0.0562, 0.0539, 0.8900],\n",
      "        [0.0549, 0.0545, 0.8906],\n",
      "        [0.0614, 0.0378, 0.9008],\n",
      "        [0.0528, 0.0510, 0.8963],\n",
      "        [0.0554, 0.0491, 0.8955],\n",
      "        [0.0549, 0.0508, 0.8943],\n",
      "        [0.1303, 0.0386, 0.8311],\n",
      "        [0.0546, 0.0495, 0.8958],\n",
      "        [0.0560, 0.0524, 0.8915],\n",
      "        [0.0734, 0.0471, 0.8795],\n",
      "        [0.0544, 0.0539, 0.8917],\n",
      "        [0.0576, 0.0495, 0.8929],\n",
      "        [0.0521, 0.0484, 0.8995],\n",
      "        [0.0565, 0.0553, 0.8882],\n",
      "        [0.0521, 0.0537, 0.8942],\n",
      "        [0.0568, 0.0547, 0.8885],\n",
      "        [0.0711, 0.0424, 0.8865],\n",
      "        [0.0542, 0.0548, 0.8910],\n",
      "        [0.0608, 0.0519, 0.8873],\n",
      "        [0.0579, 0.0387, 0.9034],\n",
      "        [0.0570, 0.0470, 0.8960],\n",
      "        [0.0672, 0.0507, 0.8821],\n",
      "        [0.6165, 0.0444, 0.3391],\n",
      "        [0.0875, 0.0473, 0.8652],\n",
      "        [0.5957, 0.0449, 0.3594],\n",
      "        [0.0641, 0.0622, 0.8737],\n",
      "        [0.6688, 0.0432, 0.2880],\n",
      "        [0.0878, 0.0526, 0.8596],\n",
      "        [0.0547, 0.0474, 0.8979],\n",
      "        [0.0580, 0.0507, 0.8913],\n",
      "        [0.0637, 0.0513, 0.8850],\n",
      "        [0.0553, 0.0498, 0.8949],\n",
      "        [0.0559, 0.0563, 0.8878],\n",
      "        [0.0576, 0.0614, 0.8810],\n",
      "        [0.0538, 0.0507, 0.8956],\n",
      "        [0.0536, 0.0461, 0.9004],\n",
      "        [0.0547, 0.0436, 0.9017],\n",
      "        [0.0614, 0.0515, 0.8871],\n",
      "        [0.1187, 0.0372, 0.8441],\n",
      "        [0.0772, 0.0393, 0.8834],\n",
      "        [0.0575, 0.0518, 0.8907],\n",
      "        [0.0574, 0.0566, 0.8860],\n",
      "        [0.0551, 0.0439, 0.9010],\n",
      "        [0.0561, 0.0569, 0.8870],\n",
      "        [0.0529, 0.0552, 0.8919],\n",
      "        [0.0586, 0.0529, 0.8885],\n",
      "        [0.0601, 0.0581, 0.8817]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep-learning/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2b798edc504cc885a883703bcd362b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.024 MB of 0.024 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁███████</td></tr><tr><td>train/accuracy_epoch</td><td>▁</td></tr><tr><td>train/accuracy_step</td><td>▁▃▅▅▅▆▇▆▇▇▇▇▇██▇▆█▇█▇▇▇▇▇█▇▇██████▇██▇██</td></tr><tr><td>train/f1_epoch</td><td>▁</td></tr><tr><td>train/f1_step</td><td>▁▃▅▅▅▆▇▆▇▇▇▇▇██▇▆█▇█▇▇▇▇▇█▇▇██████▇██▇██</td></tr><tr><td>train/loss</td><td>█▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▁▂▁▁▂▁▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▁▁▁▇▇▇▇▇▇██</td></tr><tr><td>val/accuracy_epoch</td><td>▁</td></tr><tr><td>val/accuracy_step</td><td>▁</td></tr><tr><td>val/f1_epoch</td><td>▁</td></tr><tr><td>val/f1_step</td><td>▁</td></tr><tr><td>val/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train/accuracy_epoch</td><td>0.85382</td></tr><tr><td>train/accuracy_step</td><td>1.0</td></tr><tr><td>train/f1_epoch</td><td>0.85382</td></tr><tr><td>train/f1_step</td><td>1.0</td></tr><tr><td>train/loss</td><td>0.10217</td></tr><tr><td>trainer/global_step</td><td>41</td></tr><tr><td>val/accuracy_epoch</td><td>0.96591</td></tr><tr><td>val/accuracy_step</td><td>0.96591</td></tr><tr><td>val/f1_epoch</td><td>0.96591</td></tr><tr><td>val/f1_step</td><td>0.96591</td></tr><tr><td>val/loss</td><td>0.16523</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-water-47</strong> at: <a href='https://wandb.ai/yvokeller/data-chatbot/runs/2t5q4vb4' target=\"_blank\">https://wandb.ai/yvokeller/data-chatbot/runs/2t5q4vb4</a><br/>Synced 6 W&B file(s), 4 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_170340-2t5q4vb4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Config\n",
    "run_config = {\n",
    "    'epochs': 6,\n",
    "    'max_len': None,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "# Load training and validation data\n",
    "train_df = pd.read_parquet('../data/train.parquet')\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "print('train', train_df.shape)\n",
    "train_dataloader = get_dataloader(train_df, tokenizer, max_len=run_config.get('max_len'), batch_size=run_config.get('batch_size'))\n",
    "\n",
    "val_df = pd.read_parquet('../data/test.parquet')\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "print('val', val_df.shape)\n",
    "val_dataloader = get_dataloader(val_df, tokenizer, run_config.get('max_len'), batch_size=run_config.get('batch_size'), shuffle=False, nobatch=True)\n",
    "\n",
    "model = finetune_bert(run_config, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The model achieved a validation accuracy of 97.8% after training. We are concerned that it overfits on the question mark for classifying questions, which we need to investigate further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model locally\n",
    "\n",
    "For evaluation purposes, we want to persist the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert-classifier/tokenizer/tokenizer_config.json',\n",
       " 'bert-classifier/tokenizer/special_tokens_map.json',\n",
       " 'bert-classifier/tokenizer/vocab.txt',\n",
       " 'bert-classifier/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('bert-classifier/model')\n",
    "tokenizer.save_pretrained('bert-classifier/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load model locally\n",
    "loaded_model = BertForSequenceClassification.from_pretrained('bert-classifier/model', num_labels=3)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained('bert-classifier/tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model on HuggingFace\n",
    "\n",
    "Our BERT classifier for the Data Chatbot is published here: https://huggingface.co/nlpchallenges/Text-Classification/tree/main\n",
    "\n",
    "Before pushing to the hub for the first time, make sure to run `huggingface-cli login` in your terminal, and paste the API token from your HuggingFace profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21b1f2d60864b9c89b670890d648789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/711M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/nlpchallenges/Text-Classification/commit/eb0bf2fb07564a5156f0f1c507e670a896368011', commit_message='Upload tokenizer', commit_description='', oid='eb0bf2fb07564a5156f0f1c507e670a896368011', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"nlpchallenges/Text-Classification\")\n",
    "tokenizer.push_to_hub(\"nlpchallenges/Text-Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6f79c778494b4abaee68c0d644b33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3917f3efee114b2f9792af7aa1f3dc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae36849d6b345da9b3f35cc58b9bc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edeef31c7816472696b45c539d1b5e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865873a73b434631ad16351995ff41f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1096b84181f64c448dc40914b5722944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/711M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_tokenizer = BertTokenizer.from_pretrained('nlpchallenges/Text-Classification')\n",
    "loaded_model = BertForSequenceClassification.from_pretrained('nlpchallenges/Text-Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "def classify_text(user_input, model, tokenizer, max_len=100):\n",
    "    # Tokenize the user input\n",
    "    inputs = tokenizer(\n",
    "        user_input,\n",
    "        None,\n",
    "        add_special_tokens=True, # Add '[CLS]' and '[SEP]', default True\n",
    "        max_length=max_len, # Maximum length to use by one of the truncation/padding parameters\n",
    "        padding='max_length', # Pad to a maximum length specified with the argument max_length\n",
    "        truncation=True, # Truncate to a maximum length specified with the argument max_length\n",
    "    )\n",
    "\n",
    "    ids = torch.tensor(inputs['input_ids'], dtype=torch.long).unsqueeze(0) # Indices of input sequence tokens in the vocabulary\n",
    "    mask = torch.tensor(inputs['attention_mask'], dtype=torch.long).unsqueeze(0) # Mask to avoid performing attention on padding token indices\n",
    "    \n",
    "    \n",
    "    # Get model output\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(ids, attention_mask=mask)\n",
    "    \n",
    "    # Get predicted label index\n",
    "    _, predicted_idx = torch.max(output.logits, 1)\n",
    "    \n",
    "    # Map index to label\n",
    "    label_mapping = {0: 'harm', 1: 'question', 2: 'concern'}\n",
    "    return label_mapping[predicted_idx.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_text(\"Kannst du diese Frage beantworten?\", loaded_model, loaded_tokenizer, max_len=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
