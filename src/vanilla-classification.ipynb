{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Utilities\n",
    "In diesem Abschnitt befindet sich Code der zur Vorbereitung für den Hauptteil dient inklusive \n",
    "- sämtlichen `import` Statements und \n",
    "- benutzerdefinierten Funktionen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# metrics & model selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy language model, needed for preprocessing\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy language model and define pipeline\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "unwanted_pipes = [\"ner\", \"parser\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom tokenizer that removes punctuation, spaces and non-alphabetic characters\n",
    "def spacy_tokenizer(doc):\n",
    "    with nlp.disable_pipes(*unwanted_pipes):\n",
    "        return [t for t in nlp(doc) if not t.is_punct and not t.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_docs_with_character(chars, corpus):\n",
    "    docs = []\n",
    "    index = []\n",
    "    label = []\n",
    "    for doc in corpus[\"text\"]:\n",
    "        if any(char in chars for char in doc):\n",
    "            docs.append(doc)\n",
    "            index.append(corpus[corpus[\"text\"] == doc].index[0])\n",
    "            label.append(corpus[corpus[\"text\"] == doc][\"label\"].values[0])\n",
    "    return pd.DataFrame({\"text\": docs, \"index\": index, \"label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(col, data):\n",
    "    print()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(data, y=col)\n",
    "    ax1 = plt.gca()\n",
    "    plt.ylabel(col, labelpad=12.5)\n",
    "\n",
    "    # plot distribution of classes rotated 90 degrees\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    ax.set_ylim(ax1.get_ylim())\n",
    "    sns.kdeplot(data, y=col, hue=\"label\", fill=False, ax=ax)\n",
    "    plt.xlabel(\"Density\", labelpad=12.5)\n",
    "    plt.ylabel(\"\", labelpad=12.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(y_val, y_pred, labels):\n",
    "    # print clafficiation report\n",
    "    print(\"\\t\\t\\t\\tCLASSIFICATIION METRICS\\n\")\n",
    "    print(metrics.classification_report(y_val, y_pred, target_names=labels))\n",
    "\n",
    "    # plot confusion matrix\n",
    "    conf_mat = confusion_matrix(y_val, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    sns.heatmap(\n",
    "        conf_mat,\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        fmt=\"d\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "    )\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.title(\"CONFUSION MATRIX\\n\", size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übersicht der Daten\n",
    "Nachfolgend wird versucht, einen Überblick über die geladenen Daten zu schaffen. Dazu werden zunächst die Daten eingelesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_df = pd.read_parquet(\"../data/train.parquet\")\n",
    "test_df = pd.read_parquet(\"../data/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data shapes\n",
    "print(\"Die geladenen Daten verfügen über folgende Dimensionen:\")\n",
    "pd.DataFrame(\n",
    "    [[train_df.shape[0], train_df.shape[1]], [test_df.shape[0], test_df.shape[1]]],\n",
    "    columns=[\"Rows\", \"Columns\"],\n",
    "    index=[\"Train\", \"Test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die beiden Datensets werden nun zusammengefügt, damit eine einheitliche übersicht geschaffen werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test data for combined eda\n",
    "corpus = (\n",
    "    pd.concat([train_df, test_df]).sample(frac=1, random_state=4).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Der gesamte Korpus beinhaltet\",\n",
    "    corpus.shape[0],\n",
    "    \"Dokumente und\",\n",
    "    corpus.shape[1],\n",
    "    \"Spalten.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um einen ersten Eindruck zu erhalten, werden nachfolgend die ersten paar Zeilen ausgegeben. Danach werden Duplikate und fehlende Werte addressiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show missing values\n",
    "print(\"Fehlende Werte im Korpus:\")\n",
    "print(corpus.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of duplicates\n",
    "print(\"Der Korpus beinhaltet\", corpus.duplicated().sum(), \"Duplikate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "corpus = corpus.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Nach Entfernung der Duplikaten verfügt der Korpus über die folgende Dimension:\",\n",
    "    corpus.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Korpus dürfte nun frei von fehlenden Werten und Duplikaten sein. Nachfolgend zeigen diverse Grafiken weitere Aspekte des Datensatzes auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of classes\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=\"label\", data=corpus)\n",
    "plt.title(\"Verteilung der Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus dieser Grafik ist zu entnehmen, dass die Verteilung der Klassen sehr gleichmässig ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot length of text\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(corpus[\"text\"].str.len())\n",
    "plt.title(\"Verteilung der Länge der Dokumente\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der oben gezeigte Plot zeigt die Länge der Dokumente im Form eines Histogramms. Es gibt wenige Dokumente die auffällig lang oder kurz sind und die meisten Dokumente sind zwischen 40 und 80 Zeichen lang.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of unique characters in all tokens\n",
    "unique_chars = list(set([char.lower() for doc in corpus[\"text\"] for char in doc]))\n",
    "\n",
    "print(\"Die Anzahl der eindeutigen Zeichen im Korpus ist:\", len(unique_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count frequency of characters\n",
    "char_freq = dict(zip(unique_chars, [0] * len(unique_chars)))\n",
    "\n",
    "for char in unique_chars:\n",
    "    for doc in corpus[\"text\"]:\n",
    "        count = char_freq.get(char)\n",
    "        count += doc.count(char)\n",
    "        char_freq.update({char: count})\n",
    "\n",
    "char_freq = pd.DataFrame.from_dict(char_freq, orient=\"index\", columns=[\"freq\"])\n",
    "char_freq = char_freq.sort_values(by=\"freq\", ascending=False)\n",
    "\n",
    "# log scale frequency\n",
    "char_freq[\"freq\"] = char_freq[\"freq\"].apply(lambda x: np.log(x))\n",
    "\n",
    "\n",
    "# plot frequency of characters as horizontal bar chart\n",
    "plt.figure(figsize=(10, 20))\n",
    "sns.barplot(x=char_freq[\"freq\"], y=char_freq.index)\n",
    "plt.xlabel(\"log(Häufigkeit)\")\n",
    "plt.title(\"Häufigkeit der Zeichen (log skaliert)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Plot zeigt die Häufigkeit sämtlicher im Korpus enthaltenen Zeichen. Daraus zu entnehmen ist, dass einige unerwartete Sonderzeichen vorhanden sind. Diese werden nachfolgend genauer untersucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set of special characters\n",
    "special_chars = set([char for char in unique_chars if not char.isalnum()])\n",
    "print(\"Die Anzahl von speziellen Zeichen im Korpus ist: \", len(special_chars))\n",
    "special_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Liste von Sonderzeichen wurde generiert, indem nicht alphanumerische Charaktere von der Liste der eindeutig vorkommenden Zeichen exkludiert wurden. \n",
    "Welcher Funktion diese Sonderzeichen dienen und welche Implikationen diese für das weitere Vorgehen haben können wird nachfolgend untersucht. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die Anzahl von Dokumenten welche das folgende Zeichen beinhalten: .\")\n",
    "find_docs_with_character([\".\"], corpus)[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die Anzahl von Dokumenten welche das folgende Zeichen beinhalten: ?\")\n",
    "find_docs_with_character([\"?\"], corpus)[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die Anzahl von Dokumenten welche das folgende Zeichen beinhalten: !\")\n",
    "find_docs_with_character([\"!\"], corpus)[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Verteilung der Satzzeichen entspricht der Erwartung, weshalb sie einen ungewollten Bias in das Modell übertragen können. Deshalb entfernen wir bei der Tokenisierung sämtliche Satzzeichen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die Dokumente, welche das folgende Zeichen beinhalten: -\")\n",
    "find_docs_with_character([\"-\"], corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es scheint, als würde das Zeichen `-` konsequent genutzt werden, um eigenstehende Wörter zu einem längeren und spezifischeren Wort zu verbinden. Nachfolgend ein kurzer Test, um zu prüfen, wie der SpaCy Tokeniser solche Wörter tokenisiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(corpus[\"text\"][16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `-` verbundene Wörter werden als ein Token wahrgenommen, was für unseren Anwendungsbereich das gewünschte Verhalten ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Die Anzahl von Dokumenten welche das folgende Zeichen beinhalten: \"')\n",
    "find_docs_with_character(['\"'], corpus)[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Die Dokumente, welche das folgende Zeichen beinhalten: \"')\n",
    "find_docs_with_character(['\"'], corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(corpus[\"text\"][1318])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `\"` Zeichen werden vom SpaCy Tokeniser entfernt. Häufig werden in diesem Kontext Named Entities in Hochkommas gesetzt. Named Entity Recognition nutzen wir in diesem Kontext nicht, weshalb die Zeichen ohne Weiteres entfernt werden können. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die Anzahl von Dokumenten welche das folgende Zeichen beinhalten: /\")\n",
    "find_docs_with_character([\"/\"], corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(corpus[\"text\"][81])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das `/` Zeichen wird hier genutzt, um eine Auswahl von zutreffenden Wörtern zu gruppieren. Der SpaCy Tokeniser entfernt das Zeichen und teilt die Wörter in einzelne Tokens auf, was dem gewünschten Verhalten in unserem Fall entspricht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die Anzahl von Dokumenten welche das folgende Zeichen beinhalten: #\")\n",
    "find_docs_with_character([\"#\"], corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(corpus[\"text\"][416])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Zeichen `#` kommt im Korpus nur einmal vor. SpaCy's Tokenizer trennt in diesem Fall die verschiedenen Hashtags nicht in einzelne Tokens, weshalb wir nachfolgend um alle `#` weitere Leerzeichen einfügen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert space after # in strings of documents in corpus\n",
    "corpus[\"text\"] = corpus[\"text\"].apply(lambda x: x.replace(\"#\", \" # \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(corpus[\"text\"][416])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die Anzahl von Dokumenten welche das folgende Zeichen beinhalten: %\")\n",
    "find_docs_with_character([\"%\"], corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(corpus[\"text\"][1199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die Anzahl von Dokumenten welche das folgende Zeichen beinhalten: &\")\n",
    "find_docs_with_character([\"&\"], corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(corpus[\"text\"][1099])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die Anzahl von Dokumenten welche das folgende Zeichen beinhalten: :\")\n",
    "find_docs_with_character([\":\"], corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(corpus[\"text\"][836])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die Anzahl von Dokumenten welche das folgende Zeichen beinhalten: ’\")\n",
    "find_docs_with_character([\"’\"], corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(corpus[\"text\"][745])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die Anzahl von Dokumenten welche das folgende Zeichen beinhalten: '\")\n",
    "find_docs_with_character([\"'\"], corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(corpus[\"text\"][382])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(corpus[\"text\"][741])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diverse Dokumente beinhalten eines der folgenden Zeichen: `%`, `& `, `:`, `’`, `'`. Keines dieser Zeichen wird vom Tokenizer speziell beachtet, was in unserem Fall in Ordnung ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find docs with numbers\n",
    "print(\"Die Dokumente, welche Zahlen beinhalten:\")\n",
    "find_docs_with_character([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"], corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenizer(corpus[\"text\"][13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der SpaCy Tokenizer betrachtet Zahlen als Token, was in unserem Fall dem gewünschten Verhalten entspricht. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spellchecker\n",
    "Die im Korpus vorhandenen Wörter können Schreibfehler, Slang und Abkürzungen beinhalten. Den Text weiter zu normalisieren unterlassen wir anbetracht dessen, dass die Vektorisierung mittels TF-IDF nicht sehr gut mit OOV Wörtern umgehen kann. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisation (remove punctuation and spaces)\n",
    "tokens = corpus[\"text\"].apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = pd.DataFrame()\n",
    "token_df[\"doc\"] = corpus[\"text\"]\n",
    "token_df[\"tokens\"] = tokens\n",
    "token_df[\"word count\"] = token_df[\"tokens\"].apply(len)\n",
    "token_df[\"doc length\"] = corpus[\"text\"].apply(len)\n",
    "token_df[\"mean word length\"] = token_df[\"doc length\"] / token_df[\"word count\"]\n",
    "token_df[\"label\"] = corpus[\"label\"]\n",
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Durchschnittliche Anzahl Wörter pro Dokument: \", token_df[\"word count\"].mean())\n",
    "print(\"Durchschnittliche Anzahl Zeichen pro Dokument: \", token_df[\"doc length\"].mean())\n",
    "print(\n",
    "    \"Durchschnittliche Anzahl Zeichen pro Wort: \", token_df[\"mean word length\"].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = token_df.columns.tolist()[2:5]\n",
    "for feature in features:\n",
    "    visualize(feature, token_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique tokens\n",
    "tokens_flat = list([token for doc in tokens for token in doc])\n",
    "vocabulary = list(set([token.lower_ for token in tokens_flat]))\n",
    "\n",
    "# count unique tokens\n",
    "print(\"Die Anzahl von eindeutigen Tokens ist: \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_tokens = [token.lower_ for doc in tokens for token in doc]\n",
    "\n",
    "# get most common tokens\n",
    "print(\"Die meistvorkommenden Tokens sind: \")\n",
    "\n",
    "pd.DataFrame(\n",
    "    Counter(lowercase_tokens).most_common(20), columns=[\"Token\", \"Frequency\"]\n",
    ").set_index(\"Token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wordcloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400, background_color=\"white\", max_words=200\n",
    ").generate(\" \".join(lowercase_tokens))\n",
    "\n",
    "# show wordcloud\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group tokens by label and get most common tokens\n",
    "grouped_tokens = token_df.groupby(\"label\")[\"tokens\"].apply(list)\n",
    "grouped_tokens = grouped_tokens.apply(\n",
    "    lambda x: [token.lower_ for doc in x for token in doc]\n",
    ")\n",
    "grouped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_most_frequent = pd.DataFrame(\n",
    "    Counter(grouped_tokens[\"other\"]).most_common(20),\n",
    "    columns=[\"Token\", \"Frequency\"],\n",
    ").set_index(\"Token\")\n",
    "\n",
    "# plot most common tokens for other\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=other_most_frequent[\"Frequency\"], y=other_most_frequent.index)\n",
    "plt.title(\"Meistvorkommende Tokens des Labels other\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_most_frequent = pd.DataFrame(\n",
    "    Counter(grouped_tokens[\"question\"]).most_common(20),\n",
    "    columns=[\"Token\", \"Frequency\"],\n",
    ").set_index(\"Token\")\n",
    "\n",
    "# plot most common tokens for other\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=other_most_frequent[\"Frequency\"], y=other_most_frequent.index)\n",
    "plt.title(\"Meistvorkommende Tokens des Labels question\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_most_frequent = pd.DataFrame(\n",
    "    Counter(grouped_tokens[\"concern\"]).most_common(20),\n",
    "    columns=[\"Token\", \"Frequency\"],\n",
    ").set_index(\"Token\")\n",
    "\n",
    "# plot most common tokens for other\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=other_most_frequent[\"Frequency\"], y=other_most_frequent.index)\n",
    "plt.title(\"Meistvorkommende Tokens des Labels concern\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy stopword list\n",
    "stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
    "\n",
    "# check if stopwords are in top 20 most common tokens\n",
    "print(\"Stopwörter, welche in den top 20 Tokens vorkommen: \")\n",
    "for token in stopwords:\n",
    "    if token in [token for token, freq in Counter(lowercase_tokens).most_common(20)]:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.label\n",
    "y_val = test_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tf-idf vectors\n",
    "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df.text).toarray()\n",
    "X_val = vectorizer.transform(test_df.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_tokenizer(doc):\n",
    "    with nlp.disable_pipes(*unwanted_pipes):\n",
    "        return [t.lemma_ for t in nlp(doc) if not t.is_punct and not t.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tf-idf vectors\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemma_tokenizer)\n",
    "\n",
    "X_train_lemma = vectorizer.fit_transform(train_df.text).toarray()\n",
    "X_val_lemma = vectorizer.transform(test_df.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemma.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models\n",
    "With the TF-IDF scored as our features we then go on to find the best classification model for our task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In this comparison we included the following classifiers from sklearn:\n",
    "- Random Forest\n",
    "- Linear SVC\n",
    "- Multinomial Naive Bayes\n",
    "- Hist Gradient Boosting\n",
    "- Gaussian Naive Bayes\n",
    "- Linear Discriminant Analysis\n",
    "- Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models to compare\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    HistGradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "# 5 Cross-validation\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, X_train, y_train, scoring=\"accuracy\", cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=[\"model_name\", \"fold_idx\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cv_df.groupby(\"model_name\").accuracy.mean()\n",
    "std_accuracy = cv_df.groupby(\"model_name\").accuracy.std()\n",
    "\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis=1, ignore_index=True)\n",
    "acc.columns = [\"Mean Accuracy\", \"Standard deviation\"]\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(\n",
    "        model, X_train_lemma, y_train, scoring=\"accuracy\", cv=CV\n",
    "    )\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "cv_df_lemma = pd.DataFrame(entries, columns=[\"model_name\", \"fold_idx\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cv_df_lemma.groupby(\"model_name\").accuracy.mean()\n",
    "std_accuracy = cv_df_lemma.groupby(\"model_name\").accuracy.std()\n",
    "\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis=1, ignore_index=True)\n",
    "acc.columns = [\"Mean Accuracy\", \"Standard deviation\"]\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemma_tokenizer, stop_words=stopwords)\n",
    "\n",
    "X_train_wo_sw = vectorizer.fit_transform(train_df.text).toarray()\n",
    "X_val = vectorizer.transform(test_df.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(\n",
    "        model, X_train_wo_sw, y_train, scoring=\"accuracy\", cv=CV\n",
    "    )\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "cv_df_wo_sw = pd.DataFrame(entries, columns=[\"model_name\", \"fold_idx\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cv_df_wo_sw.groupby(\"model_name\").accuracy.mean()\n",
    "std_accuracy = cv_df_wo_sw.groupby(\"model_name\").accuracy.std()\n",
    "\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis=1, ignore_index=True)\n",
    "acc.columns = [\"Mean Accuracy\", \"Standard deviation\"]\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_lemma\n",
    "X_val = X_val_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit LinearSVC model to predict y values of X_val\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "model_evaluation(y_val, y_pred, [\"other\", \"question\", \"concern\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca to reduce dimensionality\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_lemma)\n",
    "X_val_pca = pca.transform(X_val_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "model.fit(X_train_pca, y_train)\n",
    "y_pred = model.predict(X_val_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(y_val, y_pred, train_df.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random kitchen sink to increase dimensionality\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "rbf_feature = RBFSampler(gamma=1, random_state=1)\n",
    "X_train_rbf = rbf_feature.fit_transform(X_train)\n",
    "X_val_rbf = rbf_feature.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "model.fit(X_train_rbf, y_train)\n",
    "y_pred = model.predict(X_val_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "model_evaluation(y_val, y_pred, train_df.label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model performance with the following pipeline:\n",
    "- Tokenize\n",
    "- Remove punctuations and spaces\n",
    "- .isalpha i can try again\n",
    "- pca to recude dimensionality but keep 95% variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show wrong predictions\n",
    "test_df[\"predicted\"] = y_pred\n",
    "test_df[test_df.label != test_df.predicted].head(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/gaussian_nb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
